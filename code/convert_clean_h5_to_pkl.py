"""
Convert cleaned HDF5 BOLD data to Parcellated Pickle files.

Input:  HDF5 files generated by clean.py (Voxel-wise, Cleaned, Sliced)
Process: Apply Schaefer Atlas using neuromaps Parcellater
Output: Pickle files containing parcellated time series
"""

import pickle
import h5py
import numpy as np
import nibabel as nib
from pathlib import Path
from argparse import ArgumentParser
from tqdm import tqdm
from neuromaps.parcellate import Parcellater
from util.atlas import Atlas
try:
    from constants import SUBS
except ImportError:
    print("Warning: constants.py not found. 'SUBS' variable is unavailable.")
    SUBS = {}

# デフォルトパス設定
CLEAN_DATA_ROOT = "/home/s-kawashima/research/output/derivatives/clean"
PARCEL_OUTPUT_ROOT = "/home/s-kawashima/research/output/parcellated"

# fsaverage6 の頂点数
N_VERTICES_FSAVERAGE6 = 81924
N_VERTICES_PER_HEMI = 40962

# グローバルキャッシュ
_parcellater_cache = {}


def get_parcellater(parcels=1000, networks=7, kong=False, space='fsaverage6'):
    """
    Atlas.schaeferからlabel_imgを取得し、Parcellaterを作成する
    """
    kong_str = "Kong2022_" if kong else ""
    cache_key = f"schaefer{parcels}_{kong_str}{networks}_{space}"

    if cache_key not in _parcellater_cache:
        print(f"Loading Schaefer {parcels} parcels, {kong_str}{networks} networks for {space}...")

        # Atlas.schaeferからlabel_imgを取得
        atlas = Atlas.schaefer(
            parcels=parcels,
            networks=networks,
            kong=kong,
            space=space
        )
        label_img = atlas.label_img  # shape: (81924,)
        # 左右半球に分割してGIFTI形式に変換
        lh_labels = label_img[:N_VERTICES_PER_HEMI].astype(np.float32)
        rh_labels = label_img[N_VERTICES_PER_HEMI:].astype(np.float32)

        lh_gii = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(lh_labels)])
        rh_gii = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(rh_labels)])

        # Parcellaterを作成してfit
        parcellater = Parcellater((lh_gii, rh_gii), 'fsaverage', resampling_target=None).fit()
        _parcellater_cache[cache_key] = parcellater

        print(f"  -> Parcellater ready: {len(atlas)} parcels")

    return _parcellater_cache[cache_key]


def array_to_gifti(data: np.ndarray):
    """
    numpy配列をGIFTIタプル(左半球, 右半球)に変換
    """
    if data.ndim == 1:
        if data.shape[0] != N_VERTICES_FSAVERAGE6:
            raise ValueError(f"Expected {N_VERTICES_FSAVERAGE6} vertices, got {data.shape[0]}")
        lh_data = data[:N_VERTICES_PER_HEMI].astype(np.float32)
        rh_data = data[N_VERTICES_PER_HEMI:].astype(np.float32)

        lh_gii = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(lh_data)])
        rh_gii = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(rh_data)])

    else:
        if data.shape[0] == N_VERTICES_FSAVERAGE6:
            data = data.T
        elif data.shape[1] != N_VERTICES_FSAVERAGE6:
            raise ValueError(
                f"Data shape {data.shape} is invalid. "
                f"Expected one dimension to be {N_VERTICES_FSAVERAGE6} vertices."
            )

        n_timepoints = data.shape[0]
        lh_data = data[:, :N_VERTICES_PER_HEMI].astype(np.float32)
        rh_data = data[:, N_VERTICES_PER_HEMI:].astype(np.float32)

        lh_darrays = [nib.gifti.GiftiDataArray(lh_data[t]) for t in range(n_timepoints)]
        rh_darrays = [nib.gifti.GiftiDataArray(rh_data[t]) for t in range(n_timepoints)]

        lh_gii = nib.GiftiImage(darrays=lh_darrays)
        rh_gii = nib.GiftiImage(darrays=rh_darrays)

    return (lh_gii, rh_gii)


def load_clean_h5(sub_id: int, narrative: str, root_dir: str):
    """
    clean.py で保存された .h5 ファイルを読み込む
    """
    sid_str = f"{sub_id:03d}"
    filename = f"sub-{sid_str}_task-{narrative}_space-fsaverage6.h5"
    filepath = Path(root_dir) / f"sub-{sid_str}" / "func" / filename

    if not filepath.exists():
        raise FileNotFoundError(f"Cleaned HDF5 file not found: {filepath}")

    with h5py.File(filepath, "r") as f:
        if "bold" not in f:
            raise KeyError(f"'bold' dataset not found in {filepath}")
        data = f["bold"][...]

    return data


def process_and_save(
    sub_id: int,
    narrative: str,
    parcellater: Parcellater,
    clean_root: str,
    output_dir: str,
    space: str = 'fsaverage6',
    parcels: int = 1000,
    networks: int = 7,
    kong: bool = False
):
    """
    ロード -> パーセレーション -> 保存
    """
    # 1. Load Cleaned Voxel Data (HDF5)
    voxel_data = load_clean_h5(sub_id, narrative, clean_root)
    print(f"voxel_data.shape: {voxel_data.shape}")

    # 2. 時点ごとにParcellate
    n_timepoints = voxel_data.shape[0]
    parcel_results = []

    for t in range(n_timepoints):
        single_timepoint = voxel_data[t]  # shape (81924,)
        lh = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(single_timepoint[:N_VERTICES_PER_HEMI].astype(np.float32))])
        rh = nib.GiftiImage(darrays=[nib.gifti.GiftiDataArray(single_timepoint[N_VERTICES_PER_HEMI:].astype(np.float32))])
        result = parcellater.transform((lh, rh), 'fsaverage')
        parcel_results.append(result)

    parcel_data = np.stack(parcel_results)  # shape (n_timepoints, n_parcels)
    print(f"parcel_data.shape: {parcel_data.shape}")

    # 3. Save as Pickle
    output_path = Path(output_dir) / narrative
    output_path.mkdir(parents=True, exist_ok=True)

    kong_str = "Kong2022_" if kong else ""
    filename = f"sub-{sub_id:03d}_task-{narrative}_{space}.pkl"
    filepath = output_path / filename

    save_dict = {
        'data': parcel_data,
        'sub_id': sub_id,
        'narrative': narrative,
        'atlas_name': f'Schaefer2018_{parcels}Parcels_{kong_str}{networks}Networks',
        'shape': parcel_data.shape,
        'source': 'clean.py HDF5 output',
        'is_sliced': True
    }
    
    with open(filepath, 'wb') as f:
        pickle.dump(save_dict, f)
        
    return str(filepath), parcel_data


# ==========================================
#  Analysis Helper Functions (Loader)
# ==========================================

def load_bold_from_pkl(filepath: str | Path):
    """Pickleファイルからデータをロードする"""
    with open(filepath, 'rb') as f:
        data_dict = pickle.load(f)
    return data_dict


def get_bold_cached(
    sub_id: int,
    narrative: str,
    output_dir: str = PARCEL_OUTPUT_ROOT,
    clean_root: str = CLEAN_DATA_ROOT,
    parcellater: Parcellater = None,
    parcels: int = 1000,
    networks: int = 7,
    space: str = 'fsaverage6',
    kong: bool = False,
    force_reload: bool = False
) -> np.ndarray:
    """
    キャッシュがあればロードし、なければ作成して保存
    """
    expected_path = Path(output_dir) / narrative / f"sub-{sub_id:03d}_task-{narrative}_{space}.pkl"
    
    if expected_path.exists() and not force_reload:
        try:
            data_dict = load_bold_from_pkl(expected_path)
            return data_dict['data']
        except Exception as e:
            print(f"Warning: Failed to load cache {expected_path}: {e}. Re-generating...")
    
    if parcellater is None:
        parcellater = get_parcellater(parcels, networks, kong, space)

    try:
        _, parcel_data = process_and_save(
            sub_id=sub_id,
            narrative=narrative,
            parcellater=parcellater,
            clean_root=clean_root,
            output_dir=output_dir,
            space=space,
            parcels=parcels,
            networks=networks,
            kong=kong
        )
        return parcel_data
        
    except FileNotFoundError:
        print(f"Error: Source HDF5 not found for Subject {sub_id}, Task {narrative}.")
        return None
    except Exception as e:
        print(f"Error processing Subject {sub_id}, Task {narrative}: {e}")
        return None


# ==========================================
#  Main Execution (Batch Processing)
# ==========================================

def main():
    parser = ArgumentParser(description="Parcellate cleaned BOLD data from HDF5 to Pickle")
    
    parser.add_argument("--clean-dir", type=str, default=CLEAN_DATA_ROOT)
    parser.add_argument("--output-dir", type=str, default=PARCEL_OUTPUT_ROOT)
    parser.add_argument("--narratives", nargs="+", default=["black", "forgot", "piemanpni", "bronx"])
    parser.add_argument("--subjects", nargs="+", type=int, default=None)
    parser.add_argument("--space", type=str, default="fsaverage6")
    parser.add_argument("--parcels", type=int, default=1000)
    parser.add_argument("--networks", type=int, default=7)
    parser.add_argument("--kong", action="store_true", help="Use Kong2022 version of Schaefer atlas")
    
    args = parser.parse_args()
    
    # 1. Prepare Parcellater (Load and fit once)
    kong_str = "Kong2022 " if args.kong else ""
    print(f"Loading Schaefer {args.parcels} parcels, {kong_str}{args.networks} networks...")
    parcellater = get_parcellater(args.parcels, args.networks, args.kong, args.space)
    
    results = {'success': [], 'failed': [], 'skipped': []}
    
    # 2. Process Loop
    for narrative in args.narratives:
        if narrative not in SUBS:
            print(f"Skipping unknown narrative: {narrative}")
            continue
            
        targets = SUBS[narrative]
        if args.subjects:
            targets = [s for s in targets if s in args.subjects]
            
        print(f"\nProcessing Narrative: {narrative} ({len(targets)} subjects)")
        
        for sub_id in tqdm(targets, desc=f"  {narrative}"):
            try:
                fpath, _ = process_and_save(
                    sub_id=sub_id,
                    narrative=narrative,
                    parcellater=parcellater,
                    clean_root=args.clean_dir,
                    output_dir=args.output_dir,
                    space=args.space,
                    parcels=args.parcels,
                    networks=args.networks,
                    kong=args.kong
                )
                results['success'].append(fpath)
            except FileNotFoundError:
                results['skipped'].append((sub_id, narrative))
            except Exception as e:
                tqdm.write(f"  Error sub-{sub_id}: {e}")
                results['failed'].append((sub_id, narrative, str(e)))

    print(f"\nDone. Success: {len(results['success'])}, Failed: {len(results['failed'])}, Skipped: {len(results['skipped'])}")


if __name__ == "__main__":
    main()